<!DOCTYPE HTML>
<html>
	<head>
		<title>The ALICE Project - ALICE</title>
		<meta charset="utf-8">
		<meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="assets/css/main.css">
		<link rel="icon" type="image/x-icon" href="images/Alice icon.ico">
	</head>
	<style>
		.video-container {
			display: flex;
			justify-content: center;
			align-items: center;
		}
		.video-title {
			font-size: x-large;
			margin: 2%;
			text-align: center;
		}
		.video-wrapper {
			position: relative;
			display: inline-block;
			margin: 2%;
		}
	
		.flex {
			display: flex;
			gap: 20px;
		}
		@media screen and (max-width: 908px) {
			.flex-2 {
				flex-direction: column;
				align-items: center;
			}
			.flex-2 article {
				width: 100%;
				max-width: 600px;
				margin-bottom: 30px;
			}
			.flex-2 iframe {
				width: 100%;
				height: auto;
			}
			#banner h1 {
				font-size: medium;
			}
			#banner p {
				font-size: smaller;
			}
			#banner {
				padding: 20px 10px;
				text-align: center;
			}
			#header .header-left.logo {
				display: none;
			}
		}
	</style>

	<body>

		<!-- Header -->
		<header id="header"><div class="inner">
			<a href="index.html" class="logo">
				<div class="header-left logo">
                    <img src="images/Alice Logo.jpg" alt="Pic 01" width="80" height="80">
				</div>
				</a>
				<nav id="nav"><a href="index.html">About</a>
					<a href="alice.html">ALICE</a>
					<a href="documentation.html">Documentation</a>
					<a href="team.html">Team</a>
				</nav>
				<a href="#navPanel" class="navPanelToggle"><span class="fa fa-bars"></span></a>
			</div>
		</div>
		</header>

		<!-- Banner -->
		<section id="banner">
			<!-- <h1>Welcome to Theory</h1> -->
			<h1>Welcome to ALICE</h1>
			<p>ALICE: Aligning Language models with Interactive Code Execution</p>
		</section>

			<!-- Main -->
			<!-- <section id="main" class="wrapper"><div class="inner"> -->
			<section id="three" class="wrapper">
				<div class="inner">
					<header class="align-center">
						<h2>Introduction</h2>
					</header>
					<p>
						Large Language Models have revolutionized code generation and execution tasks, 
						significantly enhancing development efficiency, lowering barriers to coding, and improving code quality. 
						However, existing frameworks struggle to perform well in complex development cycles with various industry 
						standards that heavily rely on coordination and communication. Additionally, we need more suitable data to align the system, 
						especially in highly interactive scenarios. 
					</p>

					<p>To address these issues, we developed a multi-agent collaboration system that treats humans and LLMs as agents, enabling them to interact. 
						This system facilitates error correction before execution and allows adjustments based on existing code. 
						To enhance code performance and execution efficiency, we defined meta-agents and meta-tools, abstracting user instructions into modular 
						tasks that can be executed in parallel. 
						We aligned the system by leveraging the data we obtained to achieve continuous self-improvement. 
						Finally, we developed an efficient <b>meta-agent collaboration system</b> integrating interactive code generation and parallelizable system execution feedback, 
						which we call ALICE. 
					</p>
					<p>ALICE has several applications: </p>
					<ul>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							ALICE is able to generate high-quality data through multi-turn interactions and feedback without human intervention.
							Most importantly, we can produce data with traces from agent strategies like ReAct and Reflexion, 
							which are scarce but offer potential for aligning advanced LLMs.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							We can generate multimodal data from code based on human instructions, 
							which can be used to train a Sora-like model with modularized refinement. 
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							ALICE can develop highly interactive open-ended games, generate immersive 3D scenes in VR, and act as world simulators. 
						</li>
					</ul>
					<p>
						Currently, we focus on code generation and execution in game engines like Unity, as they provide a fully controllable, observable, and modular virtual environment. 
						We plan to generalize this system to broader domains in the future. 
					</p>
				</div>
			</section>
			
			<!-- 2 Method -->
			<section id="three" class="wrapper">
				<div class="inner">
					<header class="align-center">
						<h2>Method</h2>
					</header>

					<p>
						We propose an efficient meta-agent collaboration framework named ALICE, designed to follow user
						instructions through active communication, aligning with their intentions, and generating
						code based on continuous feedback and interaction. This section will discuss 3 main
						components of ALICE, Controller LLM, Intent LLM and Execution LLM.
					</p>
				</div>
			</section>

			<!-- 2.1 Controller LLM for Communication -->
			<section id="three" class="wrapper">
				<div class="inner">
					<b><h3>Controller LLM for Communication</h3></b>

					<p>ALICE distinguishes from traditional multi-agent collaboration frameworks 
						by its allowance of dynamic creation and management of agents and tools.</p>
					<ul>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							Instruction Prompts as Agent Creation: by
							employing a modular approach where different 
							agents are created to handle distinct parts
							of the task sequence, ALICE achieves a level
							of personalization and responsiveness that is
							more faithful to the user's intentions.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							Tools as Agent Instruction: In ALICE, tools
							(like RAG, execution and vision feedback) are not merely supplementary; 
							they are integral to how agents
							perceive and execute tasks. Tools can be used
							to create other tools that affect how each agent
							behave. Agent creation via prompt is also a
							core tool of the controller LLM.
						</li>
					</ul>
					<p>We setup a controller LLM with the following
						fixed operations that manage and interact with a
						suite of intention LLMs, which can be treated as
						general LLM agents seen in other frameworks.</p>
					<ul>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
						Create/Edit/Delete: modifying another controller's instruction prompt, creating or
						deleting a new controller by writing done its instruction prompt.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
						Route Setup: setup communication route for a new intention LLM to interact with
						other intention LLMs it controls.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
						Call: send instruction to one intention LLM under its control.
						</li>
					</ul>
					<p>
						This configuration enables the controller LLM
						to act as the creator of other LLMs, which are essentially 
						LLMs with different instruction prompts.
						Note that the LLMs it creates are also controllers,
						i.e. they are equipped with the same 3 operations
						as their parent controller. The controller LLMs do
						not do any task; instead, they are communicators
						with other LLMs and the user. 
					</p>
					<p>
						The following figure gives an example of a Controller LLM.</p>
					<div class="align-center">
						<div class="image">
							<img src="images/ALICE-Controller.png" alt="Pic 01" width="900" style="max-width: 100%; height: auto;">
						</div>
					</div>
				</div>
			</section>

			<!-- 2.2 Intent LLM for Alignment -->
			<section id="three" class="wrapper">
				<div class="inner">
					<b><h3>Intent LLM for Alignment</h3></b>

					<p>
						Intent LLMs are named by their nature of following
						personalized instructions. Each intent LLM is in
						charge of all the changes of a special code script,
						that is called a <i>game component</i>, and an execution
						LLM.
					</p>
					<p>
						<b>Game Components</b><br> Physic engines usually use a component-based architecture, where
						each object in the virtual scene can have various components attached to them that perform
						operations in parallel. This modular approach allows developers to compose behavior and
						properties by attaching different components to objects, such as renderers, scripts, colliders,
						or custom components. A game component is a script that an object can have, for example, a
						<i>Terrain.cs</i> script defines how an area of geographic structure is rendered, containing methods
						(tools) to create trees, grass and mountains. It can be treated as a class of any API library.
						As such, we attach a fine-tunable LLM to each API script we care about, by reading its
						class documentation in its declaration, attribute getter and setter, to each method (function)
						example usage as part of the model prompt.
					</p>

					<p>
						The intent LLM is equipped with the following operations.
					</p>
					<ul>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							Create/Edit/Delete: modifying tools from the API it is in charge of, for example,
							it can add or delete a RAG or a vision feedback system for the <i>Terrain.cs</i> API
							when it see fits to reduce memory overhead or boost execution LLM's performance.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							Route Setup: create inherited scripts from existing APIs, which creates new intent
							LLM, upon doing this, it must ask the controller LLM for the communication route
							of the new intention.
						</li>
						<li style="text-indent: em; padding-left: 1em; margin-left: 1em;">
							Call: call into the execution LLM with the task it should do.
						</li>
					</ul>
					<p>
						This setup allows the intent LLM to be a creator for code scripts, 
						especially higher-level code from existing code. 
						This design is analagous to the idea of skill library in <a href="https://voyager.minedojo.org/">Voyager (Wang et al., 2023)</a>,
						where the agent is prompted to adapt new tools that incorporates existing atomic tools.
					</p>
					<div class="align-center">
						<div class="image">
							<img src="images/ALICE-Intent.png" alt="Pic 01" width="900" style="max-width: 100%; height: auto;">
						</div>
					</div>
				</div>
			</section>

			<!-- 2.3 Execution LLM for Interactive Coding -->
			<section id="three" class="wrapper">
				<div class="inner">
					<b><h3>Execution LLM for Interactive Coding</h3></b>

					<p>
						An execution LLM controls how an API script should be used when receiving user instructions 
						related to its component update, augmented with tools (functions) written in the given
						script. In practice, we give it the header file (Terrain.h).
					</p>
					<p>
						Different from a general instruction-tuned code generation model, 
						an interactive code execution LLM
						generates code via multi-turn conversation feedback from
						the game engine and the user. As
						we modularize the API separation, each execution
						model can criticize the given instruction for its
						game component by asking back the parent intent
						LLM for clarification in code execution with unknown parameters. 
						For example, to add trees in a
						Terrain component, the user might want to further
						specify the tree density and range requirements
						before letting the model proceed with the code
						generation task. We collect feedback from each
						execution LLM and send them to the upper level
						to wait for the user or the controller with a broader
						knowledge about the APIs to respond.
					</p>
					<p>
						The interactive code execution LLM is equipped with the operations to generate code,
						incorporating feedback from the tools it is given from the intent LLM, or report the final
						code it writes that is ready for execution.
					</p>
				</div>
			</section>

			<!-- 2.4 Agent Strategies -->
			<section id="three" class="wrapper">
				<div class="inner">
					<header class="align-center">
						<h2>Agent Strategies</h2>
					</header>
					<p>
						A big advantage of the ALICE system is that it is orthogonal to popular agent strategies by
						its nature of design. The operations that are primarily equipped to each LLM component
						can be boosted by planning ahead, reasoning through, self-critized, and virtually executed
						as actions to better follow user instructions.
					</p>
					<p>
						To use <a href="https://react-lm.github.io/">ReAct (Yao et al., 2023)</a>, for example, the intent LLM 
						can be equipped with the following action space (tools): reasoning trace/thought,
						retrieving relevant targets/components in the simulated environment, 
						communication with the user, and finally, answering the question. 
						Essentially, at each trace stage, the agent can either make a reasoning thought, 
						generate code after retrieving relevant
						components to the task at hand, communicate with
						the user to learn further specifications, or provide a
						final answer. The tools it have is controlled by its upper-level controller LLM for efficiency and flexibility.
					</p>
					<p>
						ALICE example input and output. The <i>ALICE (icon)</i> result from left to right shows
						the result of GPT-3.5 before, as fine-tuning progress, and after alignment. The ground truth
						is not included in the training.
					</p>
					<div class="align-center">
						<div class="image">
							<img src="images/ALICE example.png" alt="Pic 01" width="900" style="max-width: 100%; height: auto;">
						</div>
					</div>
				</div>
			</section>

			<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<div class="flex">
						<div class="footer-content">
							<h2>Connect with Us</h2>
							<p>Follow us on GitHub and social media to stay updated with our latest news and offers!</p>
							<p><i><b>We are also looking for brilliant people to join the ALICE project, connect with us for more details!</b></i></p>
							<ul class="icons">
								<li><a href="https://github.com/yang-su2000/Voice2Action" class="icon fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/groups/13044244/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="mailto:ys724@cornell.edu" class="icon fa-envelope"><span class="label">Email</span></a></li>
							</ul>
						</div>
					</div>
				</div>
			</footer>
		<!-- <div class="copyright">
			Site made with: <a href="https://templated.co/">TEMPLATED.CO</a>
		</div> -->

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script><script src="assets/js/skel.min.js"></script><script src="assets/js/util.js"></script><script src="assets/js/main.js"></script></body></html>
